# [译]Efficient data transfer through zero copy

https://developer.ibm.com/articles/j-zerocopy/

很多Web应用程序提供大量的静态内容，这相当于从磁盘读取数据并将完全相同的数据写回响应套接字。这个过程需要CPU参与的部分很少，但通常效率却很低：首先内核从磁盘读取数据，并将其写入内核空间，在将数据从内核空间写入到应用程序的内存空间，然后应用程序将其推回到内核空间，并将其写到socket。实际上，应用程序充当了一个低效的中介，将数据从磁盘文件获取到套接字。

每次数据传输到内核，需要CPU参数数据拷贝，消耗CPU周期和内存总线带宽。幸运的是可以通过零拷贝(尽可能的少)技术消除数据的拷贝。使用零拷贝技术的应用可以直接将数据从磁盘拷贝到socket，无需经过应用程序做中转。领拷贝技术提升了应用程序性能而且建了而下cpu在内核态和用户态的上下文切换。

Java在unix和linux系统中通过 `java.nio.channels.FileChannel`的`transferTo()`方法支持了领拷贝。通过调用`transferTo()`方法可以直接将channel的数据传输到另外一个可以写的byte channel无需应用程序做中转。本文首先演示了通过传统的复制语义进行简单的文件传输所带来的开销，然后展示了使用“transferTo()”的零拷贝技术如何获得更好的性能。

# 数据传输：传统的复制语义

考虑如下场景：读取文件数据，通过网络传播。（这个场景描述了很多服务器的行为，包括静态web服务器，FTP服务器，邮件服务器等）。核心代码包含带清单以中，包括两个函数调用。 ([download the complete sample code](http://download.boulder.ibm.com/ibmdl/pub/software/dw/java/j-zerocopy.zip)):

##### Listing 1. Copying bytes from a file to a socket

```
File.read(fileDesc, buf, len);
Socket.send(socket, buf, len);
```

虽然清单1在概念上很简单，但是内部复制操作需要在用户模式和内核模式之间进行4次上下文切换，并且在操作完成之前要复制4次数据。图1显示了数据是如何从文件内部传输到socket的:

##### Figure 1. Traditional data copying approach

![Traditional data copying approach](https://developer.ibm.com/developer/articles/j-zerocopy/images/figure1.gif)

##### Figure 2. Traditional context switches

![Traditional context switches](https://developer.ibm.com/developer/articles/j-zerocopy/images/figure2.gif)



1. 调用`read()` 函数操内核从用户态切换到内核态(图2)。内部sys_read()(或等效)来从文件中读取数据。第一个副本(图1)由直接内存访问(DMA)引擎执行，该引擎从磁盘读取文件内容并将其存储到内核地址空间缓冲区中
2. 请求的数据从读缓冲区复制到用户缓冲区，`read()` 调用返回。调用的返回导致另一个上下文从内核切换回用户模式。现在数据存储在用户地址空间缓冲区中
3. `send()`套接字调用导致上下文从用户态切换到内核态。执行第三次复制，再次将数据放入内核地址空间缓冲区。不过，这一次，数据被放入一个不同的缓冲区，这个缓冲区与目标套接字相关联。
4. `send() `系统调用返回，导致第四次上下文切换。DMA引擎将数据从内核缓冲区传递到协议引擎，会独立地、异步地进行第四次复制。

使用中间内核缓冲区(而不是直接将数据传输到用户缓冲区)可能看起来效率很低。但是中间内核缓冲区是为了提高性能而被被引入到整个过程的。在读取端使用中间缓冲区允许内核缓冲区充当"预读缓存"。当请求的数据量小于内核缓冲区大小时，这将显著提高性能。写端上的中间缓冲区允许异步完成写操作。

不幸的是，如果请求的数据的大小远远大于内核缓冲区的大小，这种方法本身就会成为性能瓶颈。数据在最终交付给应用程序之前，会在磁盘、内核缓冲区和用户缓冲区之间复制多次。

零拷贝通过消除这些冗余的数据拷贝来提高性能。

## Data transfer: The zero-copy approach

If you re-examine the [traditional scenario](https://developer.ibm.com/articles/j-zerocopy/#trad), you’ll notice that the second and third data copies are not actually required. The application does nothing other than cache the data and transfer it back to the socket buffer. Instead, the data could be transferred directly from the read buffer to the socket buffer. The `transferTo()` method lets you do exactly this. Listing 2 shows the method signature of `transferTo()`:



##### Listing 2. The transferTo() method

```
public void transferTo(long position, long count, WritableByteChannel target);
```

Show more



The `transferTo()` method transfers data from the file channel to the given writable byte channel. Internally, it depends on the underlying operating system’s support for zero copy; in UNIX and various flavors of Linux, this call is routed to the `sendfile()` system call, shown in Listing 3, which transfers data from one file descriptor to another:



##### Listing 3. The sendfile() system call

```
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

Show more



The action of the `file.read()` and `socket.send()` calls in [Listing 1](https://developer.ibm.com/articles/j-zerocopy/#listing1) can be replaced by a single `transferTo()` call, as shown in Listing 4:



##### Listing 4. Using transferTo() to copy data from a disk file to a socket

```
transferTo(position, count, writableChannel);
```

Show more



Figure 3 shows the data path when the `transferTo()` method is used:

##### Figure 3. Data copy with transferTo()

![Data copy with transferTo()](https://developer.ibm.com/developer/articles/j-zerocopy/images/figure3.gif)

##### Figure 4. Context switching with transferTo()

![Context switching when using transferTo()](https://developer.ibm.com/developer/articles/j-zerocopy/images/figure4.gif)

1. The `transferTo()` method causes the file contents to be copied into a read buffer by the DMA engine. Then the data is copied by the kernel into the kernel buffer associated with the output socket.
2. The third copy happens as the DMA engine passes the data from the kernel socket buffers to the protocol engine.

This is an improvement: we’ve reduced the number of context switches from four to two and reduced the number of data copies from four to three (only one of which involves the CPU). But this does not yet get us to our goal of zero copy. We can further reduce the data duplication done by the kernel if the underlying network interface card supports *gather operations*. In Linux kernels 2.4 and later, the socket buffer descriptor was modified to accommodate this requirement. This approach not only reduces multiple context switches but also eliminates the duplicated data copies that require CPU involvement. The user-side usage still remains the same, but the intrinsics have changed:

1. The `transferTo()` method causes the file contents to be copied into a kernel buffer by the DMA engine.
2. No data is copied into the socket buffer. Instead, only descriptors with information about the location and length of the data are appended to the socket buffer. The DMA engine passes data directly from the kernel buffer to the protocol engine, thus eliminating the remaining final CPU copy.

Figure 5 shows the data copies using `transferTo()` with the gather operation:

##### Figure 5. Data copies when transferTo() and gather operations are used

![Data copies when transferTo() and gather operations are used](https://developer.ibm.com/developer/articles/j-zerocopy/images/figure5.gif)

Now let’s put zero copy into practice, using the same example of transferring a file between a client and a server (see [Download](https://developer.ibm.com/articles/j-zerocopy/#download) for the sample code). `TraditionalClient.java` and `TraditionalServer.java` are based on the traditional copy semantics, using `File.read()` and `Socket.send()`. `TraditionalServer.java` is a server program that listens on a particular port for the client to connect, and then reads 4K bytes of data at a time from the socket. `TraditionalClient.java` connects to the server, reads (using `File.read()`) 4K bytes of data from a file, and sends (using `socket.send()`) the contents to the server via the socket.

Similarly, `TransferToServer.java` and `TransferToClient.java` perform the same function, but instead use the `transferTo()` method (and in turn the `sendfile()` system call) to transfer the file from server to client.

### Performance comparison

We executed the sample programs on a Linux system running the 2.6 kernel and measured the run time in milliseconds for both the traditional approach and the `transferTo()` approach for various sizes. Table 1 shows the results:

##### Table 1. Performance comparison: Traditional approach vs. zero copy

| File size | Normal file transfer (ms) | transferTo (ms) |
| :-------- | :------------------------ | :-------------- |
| 7MB       | 156                       | 45              |
| 21MB      | 337                       | 128             |
| 63MB      | 843                       | 387             |
| 98MB      | 1320                      | 617             |
| 200MB     | 2124                      | 1150            |
| 350MB     | 3631                      | 1762            |
| 700MB     | 13498                     | 4422            |
| 1GB       | 18399                     | 8537            |

As you can see, the `transferTo()` API brings down the time approximately 65 percent compared to the traditional approach. This has the potential to increase performance significantly for applications that do a great deal of copying of data from one I/O channel to another, such as Web servers.

## Summary

We have demonstrated the performance advantages of using `transferTo()` compared to reading from one channel and writing the same data to another. Intermediate buffer copies — even those hidden in the kernel — can have a measurable cost. In applications that do a great deal of copying of data between channels, the zero-copy technique can offer a significant performance improvement.